# -*- coding: utf-8 -*-
"""final_fine-tuning.ipynb

Automatically generated by Colab.

"""# Importazione librerie necessarie"""

from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig,
                          DataCollatorForSeq2Seq, TrainingArguments, Trainer)
from peft import LoraConfig, get_peft_model, PeftModel
from typing import Dict
import pandas as pd
from sklearn.model_selection import GroupShuffleSplit
from sklearn.metrics import f1_score
from datasets import Dataset, DatasetDict, load_from_disk
import torch
import nltk
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from tqdm import tqdm

"""# Impostazioni fine-tuning

## Configurazione quantizzazione 8-bit
"""

bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,              # attiva quantizzazione 8-bit
    llm_int8_threshold=6.0,         # soglia per quantizzazione dinamica (default 6.0)
    llm_int8_has_fp16_weight=False  # se True, mantiene una copia FP16 per stabilità
)

"""## Caricamento modello e configurazione LoRA-Peft
Qui si imposta un modello usando come punto di partenza della configurazione LoRA i pesi addestratti nella task TSC.
"""

base_model_name = "Salesforce/codet5p-220m"
old_lora_path = "/content/drive/MyDrive/Colab Notebooks/TSC/lora-codet5p"

base_model = AutoModelForSeq2SeqLM.from_pretrained(
        base_model_name,
        quantization_config=bnb_config,    # quantizzazione per risparmiare VRAM
        device_map="auto"
    )
tokenizer = AutoTokenizer.from_pretrained(base_model_name)

model = PeftModel.from_pretrained(base_model, old_lora_path)
model = model.merge_and_unload()

new_lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q", "v","EncDecAttention.q","EncDecAttention.v"],   # nei layer di T5 si chiamano così
    lora_dropout=0.15,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)

model = get_peft_model(model, new_lora_config)
model.print_trainable_parameters()

"""## Funzioni di split e preprocessing del dataset"""

def split_dataset(df:pd.DataFrame) -> Dict[str,pd.DataFrame]:
    '''
    Funzione che prende un dataframe da usare per un training e lo suddivide in due
    subset tenendo conto della possibilità della presenza
    di più input per il medesimo output prevenendo così il dataleak.

    La suddivisione è fissata a 90% e 10%.
    '''

    groups = df["output"]

    # --- Primo split: chunk_1 (90%) vs chunk_2 (10%) ---
    splitter1 = GroupShuffleSplit(test_size=0.1, n_splits=1, random_state=42)
    chunk_1_idx, chunk_2_idx = next(splitter1.split(df, groups=groups))

    chunk_1 = df.iloc[chunk_1_idx]
    chunk_2 = df.iloc[chunk_2_idx]

    print("chunk_90:\n", len(chunk_1), "\n")
    print("chunk_10:\n", len(chunk_2), "\n")

    dataset = {'chunk_90': chunk_1, 'chunk_10': chunk_2}

    return dataset

def preprocess(batch):
    model_inputs = tokenizer(batch["input"],
                             max_length=256, truncation=True, padding=False)
    labels = tokenizer(batch["output"],
                       max_length=256, truncation=True, padding=False)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

"""## Tokenizzazione datasets training e validation"""

df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/datasets/lcquad2_train.json')
df['input'] = 'text-to-sparql: ' + df['input'].astype(str)

train_df, valid_df = split_dataset(df).values()

# converto ciascun DataFrame in un Dataset Hugging Face
train_dataset = Dataset.from_pandas(train_df)
valid_dataset = Dataset.from_pandas(valid_df)

# creo un DatasetDict con due split
dataset = DatasetDict({
    "train": train_dataset,
    "validation": valid_dataset
})

tokenized_dataset = dataset.map(preprocess, batched=True)

# Salvo il dataset tokenizzato
tokenized_dataset.save_to_disk("/content/drive/MyDrive/Colab Notebooks/datasets/tokenized_dataset")

"""## Definizione DataCollator"""

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

"""## Impostazione TrainingArguments"""

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Colab Notebooks/final_fine-tuning/results",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=1e-5,
    num_train_epochs=1,
    fp16=True,
    logging_steps=50,
    save_strategy="steps",
    save_steps=500,
    eval_strategy="steps",
    eval_steps=500,
    warmup_steps=20,
    save_total_limit=2,
    report_to="none"
)

"""## Impostazione Trainer"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    processing_class=tokenizer,
    data_collator=data_collator,
)

"""## Avvio training e salvataggio modello e tokenizer"""

trainer.train()
merged_model.save_pretrained("/content/drive/MyDrive/Colab Notebooks/modello_traduzione_tts")
tokenizer.save_pretrained("/content/drive/MyDrive/Colab Notebooks/modello_traduzione_tts")

# Salvataggio modello in f16 con ultimi pesi lora
base_model_name = "Salesforce/codet5p-220m"
tsc_lora = "/content/drive/MyDrive/Colab Notebooks/TSC/lora-codet5p"
tts_lora = "/content/drive/MyDrive/Colab Notebooks/modello_traduzione_tts"

# 1. Carica modello base in FP16 (IMPORTANTE: nessuna quantizzazione qui)
model = AutoModelForSeq2SeqLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,  # FP16
    device_map="auto"
)

# 2. Carica pesi LoRA addestramento su TSC
model = PeftModel.from_pretrained(model, tsc_lora)
model = model.merge_and_unload()

# 3. Carica pesi LoRA dell’ultimo checkpoint
model = PeftModel.from_pretrained(model, tts_lora)
merged_model = model.merge_and_unload()

save_dir = "/content/drive/MyDrive/Colab Notebooks/modello_traduzione_tts_fp16"

merged_model.save_pretrained(save_dir)
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.save_pretrained(save_dir)

"""# Testing

## Caricamento dataset di test
"""

df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/datasets/lcquad2_test.json')
df['input'] = 'text-to-sparql: ' + df['input'].astype(str)

test_dataset = Dataset.from_pandas(df)

save_dir = "/content/drive/MyDrive/Colab Notebooks/modello_traduzione_tts_fp16"
tokenizer = AutoTokenizer.from_pretrained(save_dir)
tokenized_test_dataset = test_dataset.map(preprocess, batched=True)
# Salvo il dataset tokenizzato
tokenized_test_dataset.save_to_disk("/content/drive/MyDrive/Colab Notebooks/datasets/tokenized_test_dataset")

"""## Importazione test dataset pre-tokenizzato"""

tokenized_test_dataset = load_from_disk("/content/drive/MyDrive/Colab Notebooks/datasets/tokenized_test_dataset")
print(tokenized_test_dataset)
small_eval = tokenized_test_dataset.select(range(100))

"""## Definizione oggetto per valutazione modello"""

# Scarica risorse NLTK necessarie
nltk.download('punkt', quiet=True)

class CodeT5Evaluator:
    def __init__(self, model_path):
        """
        Inizializza l'evaluator con il modello fine-tuned con LoRA

        Args:
            model_path: path alla cartella del modello e tokenizer
        """

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # Carica tokenizer
        print(f"Loading tokenizer from {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)

        # Carica modello
        print(f"Loading model from {model_path}")
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

        self.model.to(self.device)
        self.model.eval()

    def generate_predictions(self, test_dataset, batch_size=8, max_length=256):
        """
        Genera predizioni per il dataset di test

        Args:
            test_dataset: dataset tokenizzato con colonne 'input_ids' e 'labels'
            batch_size: dimensione del batch
            max_length: lunghezza massima della generazione

        Returns:
            predictions: lista di stringhe predette
            references: lista di stringhe di riferimento
        """
        predictions = []
        references = []

        # Processa in batch
        for i in tqdm(range(0, len(test_dataset), batch_size), desc="Generating"):
            batch = test_dataset[i:i+batch_size]

            # Padding dinamico del batch
            max_input_len = max(len(ids) for ids in batch['input_ids'])
            padded_input_ids = []
            padded_attention_mask = []

            for ids, mask in zip(batch['input_ids'], batch['attention_mask']):
                padding_length = max_input_len - len(ids)
                padded_input_ids.append(ids + [self.tokenizer.pad_token_id] * padding_length)
                padded_attention_mask.append(mask + [0] * padding_length)

            # Prepara input
            input_ids = torch.tensor(padded_input_ids).to(self.device)
            attention_mask = torch.tensor(padded_attention_mask).to(self.device)

            # Genera predizioni
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=max_length,
                    num_beams=5,  # Beam search per migliori risultati
                    early_stopping=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            # Decodifica predizioni
            batch_preds = self.tokenizer.batch_decode(
                outputs, skip_special_tokens=True
            )
            predictions.extend(batch_preds)

            # Decodifica references con padding dinamico
            max_label_len = max(len(label) for label in batch['labels'])
            padded_labels = []

            for label in batch['labels']:
                # Sostituisci -100 con pad_token_id
                cleaned_label = [l if l != -100 else self.tokenizer.pad_token_id for l in label]
                padding_length = max_label_len - len(cleaned_label)
                padded_labels.append(cleaned_label + [self.tokenizer.pad_token_id] * padding_length)

            batch_refs = self.tokenizer.batch_decode(
                padded_labels, skip_special_tokens=True
            )
            references.extend(batch_refs)

        return predictions, references

    def compute_bleu(self, predictions, references):
        """
        Calcola BLEU score

        Returns:
            dict con BLEU-1, BLEU-2, BLEU-3, BLEU-4
        """
        # Tokenizza per BLEU
        refs_tokenized = [[ref.split()] for ref in references]
        preds_tokenized = [pred.split() for pred in predictions]

        smoothing = SmoothingFunction().method1

        bleu_scores = {}
        for n in range(1, 5):
            weights = tuple([1.0/n] * n + [0.0] * (4-n))
            bleu = corpus_bleu(
                refs_tokenized,
                preds_tokenized,
                weights=weights,
                smoothing_function=smoothing
            )
            bleu_scores[f'BLEU-{n}'] = bleu * 100  # Converti in percentuale

        return bleu_scores

    def compute_token_f1(self, predictions, references):
        """
        Calcola F1 score a livello di token

        Returns:
            dict con F1 micro, macro e weighted
        """
        # Tokenizza
        all_pred_tokens = []
        all_ref_tokens = []

        for pred, ref in zip(predictions, references):
            pred_tokens = pred.split()
            ref_tokens = ref.split()

            # Allinea le lunghezze
            max_len = max(len(pred_tokens), len(ref_tokens))
            pred_tokens += ['<PAD>'] * (max_len - len(pred_tokens))
            ref_tokens += ['<PAD>'] * (max_len - len(ref_tokens))

            all_pred_tokens.extend(pred_tokens)
            all_ref_tokens.extend(ref_tokens)

        # Calcola F1
        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        all_tokens = list(set(all_pred_tokens + all_ref_tokens))
        le.fit(all_tokens)

        pred_encoded = le.transform(all_pred_tokens)
        ref_encoded = le.transform(all_ref_tokens)

        f1_scores = {
            'F1-micro': f1_score(ref_encoded, pred_encoded, average='micro') * 100,
            'F1-macro': f1_score(ref_encoded, pred_encoded, average='macro') * 100,
            'F1-weighted': f1_score(ref_encoded, pred_encoded, average='weighted') * 100
        }

        return f1_scores

    def compute_exact_match(self, predictions, references):
        """Calcola Exact Match Accuracy"""
        matches = sum(1 for p, r in zip(predictions, references) if p.strip() == r.strip())
        return (matches / len(predictions)) * 100

    def evaluate(self, test_dataset, batch_size=8, max_length=256, save_predictions=True):
        """
        Esegue valutazione completa

        Returns:
            dict con tutte le metriche
        """
        print("\n" + "="*50)
        print("Starting Evaluation")
        print("="*50)

        # Genera predizioni
        predictions, references = self.generate_predictions(
            test_dataset, batch_size, max_length
        )

        # Salva predizioni (opzionale)
        if save_predictions:
            with open('predictions.txt', 'w', encoding='utf-8') as f:
                for pred, ref in zip(predictions, references):
                    f.write(f"REFERENCE: {ref}\n")
                    f.write(f"PREDICTION: {pred}\n")
                    f.write("-" * 80 + "\n")
            print("\nPredictions saved to 'predictions.txt'")

        # Calcola metriche
        print("\nComputing metrics...")
        bleu_scores = self.compute_bleu(predictions, references)
        f1_scores = self.compute_token_f1(predictions, references)
        exact_match = self.compute_exact_match(predictions, references)

        # Combina risultati
        results = {
            **bleu_scores,
            **f1_scores,
            'Exact-Match': exact_match,
            'num_samples': len(predictions)
        }

        # Stampa risultati
        print("\n" + "="*50)
        print("EVALUATION RESULTS")
        print("="*50)
        for metric, score in results.items():
            if metric != 'num_samples':
                print(f"{metric:15s}: {score:6.2f}%")
            else:
                print(f"{metric:15s}: {score}")
        print("="*50)

        return results, predictions, references


def main():

    # 1. Path modello da testare
    MODEL = "/content/drive/MyDrive/Colab Notebooks/modello_traduzione_tts_fp16"

    evaluator = CodeT5Evaluator(model_path=MODEL)

    # 2. Carica il dataset di test
    # Opzione A: Dataset già tokenizzato salvato localmente
    test_dataset = load_from_disk("/content/drive/MyDrive/Colab Notebooks/datasets/tokenized_test_dataset")

    # 3. Esegui la valutazione
    results, predictions, references = evaluator.evaluate(
        test_dataset=test_dataset,
        batch_size=8,  # Riduci se hai problemi di memoria
        max_length=256,
        save_predictions=True
    )

    # 4. (Opzionale) Analizza alcuni esempi
    print("\n" + "="*50)
    print("SAMPLE PREDICTIONS")
    print("="*50)
    for i in range(min(3, len(predictions))):
        print(f"\nExample {i+1}:")
        print(f"Reference:  {references[i][:100]}...")
        print(f"Prediction: {predictions[i][:100]}...")

    return results

"""## Avvio valutazione modello"""

if __name__ == "__main__":
    # Assicurati di avere abbastanza memoria
    torch.cuda.empty_cache()

    # Esegui valutazione
    results = main()

"""# Inferenza manuale"""

inputs = tokenizer("text-to-sparql: Who is the  {country} for {head of state} of {Mahmoud Abbas} [SEP] wd:Mahmoud Abbas wd:country [SEP] wdt:instance of wdt:head of state", return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_length=128,
    num_beams=4,
    early_stopping=True
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))