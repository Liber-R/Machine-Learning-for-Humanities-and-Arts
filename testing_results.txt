LC-QUAD v2
===============================================================
EVALUATION RESULTS -> per fine-tuning per task traduzione (TTS)
===============================================================
BLEU-1         :  95.01%
BLEU-2         :  91.64%
BLEU-3         :  88.75%
BLEU-4         :  86.16%
F1-micro       :  88.19%
F1-macro       :  77.16%
F1-weighted    :  88.14%
Exact-Match    :  54.68%
num_samples    : 6028

===============================================================

================================================================================================
EVALUATION RESULTS -> per fine-tuning per task correzione triple + traduzione sparql (TSC + TTS)
================================================================================================
BLEU-1         :  93.62%
BLEU-2         :  89.78%
BLEU-3         :  86.58%
BLEU-4         :  83.71%
F1-micro       :  84.99%
F1-macro       :  72.97%
F1-weighted    :  85.13%
Exact-Match    :  51.56%
num_samples    : 6028

================================================================================================