# -*- coding: utf-8 -*-
"""TSC_fine_tuning.ipynb

Automatically generated by Colab.
"""

import os
import json
import re
import torch
import pandas as pd
from typing import Tuple, List, Dict
from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig,
                          DataCollatorForSeq2Seq, TrainingArguments, Trainer)
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig
from sklearn.model_selection import GroupShuffleSplit
from datasets import Dataset, DatasetDict, load_from_disk
from tqdm import tqdm

model_name = "Salesforce/codet5p-220m"
tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map="auto")

config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q", "v","EncDecAttention.q","EncDecAttention.v"],   # nei layer di T5 si chiamano cos√¨
    lora_dropout=0.15,
    bias="none",
    task_type="SEQ_2_SEQ_LM"
)

model = get_peft_model(model, config)
model.print_trainable_parameters()

def split_dataset(df:pd.DataFrame) -> Dict[str,pd.DataFrame]:
    '''
    Funzione che prende un dataframe da usare per un training e lo suddivide in due
    subset tenendo conto della possibilit√† della presenza
    di pi√π input per il medesimo output prevenendo cos√¨ il dataleak.

    La suddivisione √® fissata a 90% e 10%.
    '''

    groups = df["output"]

    # --- Primo split: chunk_1 (90%) vs chunk_2 (10%) ---
    splitter1 = GroupShuffleSplit(test_size=0.1, n_splits=1, random_state=42)
    chunk_1_idx, chunk_2_idx = next(splitter1.split(df, groups=groups))

    chunk_1 = df.iloc[chunk_1_idx]
    chunk_2 = df.iloc[chunk_2_idx]

    print("chunk_90:\n", len(chunk_1), "\n")
    print("chunk_10:\n", len(chunk_2), "\n")

    dataset = {'chunk_90': chunk_1, 'chunk_10': chunk_2}

    return dataset

def preprocess(batch):
    model_inputs = tokenizer(batch["input"],
                             max_length=256, truncation=True, padding=False)
    labels = tokenizer(batch["output"],
                       max_length=256, truncation=True, padding=False)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/datasets/TSC/LC-QuAD2/train.json')
df['input'] = 'sparql-to-sparql: ' + df['input'].astype(str)

train_df, valid_df = split_dataset(df).values()

# converto ciascun DataFrame in un Dataset Hugging Face
train_dataset = Dataset.from_pandas(train_df)
valid_dataset = Dataset.from_pandas(valid_df)

# creo un DatasetDict con due split
dataset = DatasetDict({
    "train": train_dataset,
    "validation": valid_dataset
})

tokenized_dataset = dataset.map(preprocess, batched=True)

# Salvo il dataset tokenizzato
tokenized_dataset.save_to_disk("/content/drive/MyDrive/Colab Notebooks/datasets/TSC/LC-QuAD2/tokenized_dataset")

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Colab Notebooks/TSC/results",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    fp16=True,
    logging_steps=50,
    save_strategy="steps",
    save_steps=500,
    eval_strategy="steps",
    eval_steps=500,
    warmup_steps=100,
    save_total_limit=2,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    processing_class=tokenizer,
    data_collator=data_collator,
)

trainer.train()
model.save_pretrained("/content/drive/MyDrive/Colab Notebooks/TSC/lora-codet5p")
tokenizer.save_pretrained("/content/drive/MyDrive/Colab Notebooks/TSC/lora-codet5p")

"""# Testing"""

bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,              # attiva quantizzazione 8-bit
    llm_int8_threshold=6.0,         # soglia per quantizzazione dinamica (default 6.0)
    llm_int8_has_fp16_weight=False  # se True, mantiene una copia FP16 per stabilit√†
)

# carica la config LoRA
peft_model_id = "/content/drive/MyDrive/Colab Notebooks/TSC/lora-codet5p"
config = PeftConfig.from_pretrained(peft_model_id)

# carica il modello base e applica LoRA
model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config)
model = PeftModel.from_pretrained(model, peft_model_id)

tokenizer = AutoTokenizer.from_pretrained(peft_model_id)

df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/datasets/TSC/LC-QuAD2/test.json')
df['input'] = 'sparql-to-sparql: ' + df['input'].astype(str)

test_dataset = Dataset.from_pandas(df)

tokenized_test_dataset = test_dataset.map(preprocess, batched=True)
# Salvo il dataset tokenizzato
tokenized_test_dataset.save_to_disk("/content/drive/MyDrive/Colab Notebooks/datasets/TSC/LC-QuAD2/tokenized_test_dataset")

class Triple:
    """Represents a SPARQL triple"""
    subject: str
    predicates: tuple
    objects: tuple

    def __init__(self, subject, predicates, objects):
      self.subject = subject
      self.predicates = predicates
      self.objects = objects

    def __hash__(self):
        return hash((self.subject, self.predicates, self.objects))

    def __eq__(self, other):
        return (self.subject == other.subject and
                self.predicates == other.predicates and
                self.objects == other.objects)


class EvaluationMetrics:
    """Stores evaluation metrics"""
    def __init__(self,
        triple_exact_match: float,
        correction_accuracy: float,
        preservation_accuracy: float,
        precision: float,
        recall: float,
        f1_score: float,
        query_exact_match: float,
        total_triples: int,
        correct_triples: int,
        flipped_triples_in_input: int,
        correctly_fixed: int,
        incorrectly_fixed: int,
        false_swaps: int,
        missed_flips: int,
        perfect_queries: int,
        total_queries: int):

        self.triple_exact_match = triple_exact_match
        self.correction_accuracy = correction_accuracy
        self.preservation_accuracy = preservation_accuracy
        self.precision = precision
        self.recall = recall
        self.f1_score = f1_score
        self.query_exact_match = query_exact_match
        self.total_triples = total_triples
        self.correct_triples = correct_triples
        self.flipped_triples_in_input = flipped_triples_in_input
        self.correctly_fixed = correctly_fixed
        self.incorrectly_fixed = incorrectly_fixed
        self.false_swaps = false_swaps
        self.missed_flips = missed_flips
        self.perfect_queries = perfect_queries
        self.total_queries = total_queries



class SPARQLTripleEvaluator:
    """Evaluates triple structure correction in SPARQL queries"""

    def retrieve_triples(self, sparql_q) -> List[str]:
        '''
        Funzione per ricavare le triple presenti nella query.

        Segue descrizione regex usata:
        triples_pattern individua triple comprendendo:
                         - variabili normalizzate introdotte da ?;
                         - literals con anche la specificazione della
                         lingua, e.g. "cane"@it;
                         - entit√† con prefisso, e.g. wd:Q123;
                         - propriet√† rdf:type abbreviata, e.g. a;
                         - propriet√† con prefisso, e.g. dbr:nato_in;
                         - triple estese da operatori AND (;), e.g.
                          ?var1 dbr:figlio_di dbo:Dio ; dbr:nato_in dbo:Nazareth
        VALUES_pattern individua clausole VALUES, e.g. VALUES ?var0 { wd:Q123 wd:Q987}

        Input: sparql_q=stringa della query
        Output: Lista di stringhe dato che non ci sono gruppi di cattura nel pattern usato
        '''

        VALUES_pattern = r'VALUES\s+\?\S+\s+\{(.*?)\}'
        triples_pattern = r'(?:[^\s\{\[\(]+:\S+|\?\S+)\s+(?:[^\s\{\[\(]+:[^\s\}\]\)]+|a|\?\S+)\s+(?:\S+:[^\s\}\]\)\.]+|\?\S+|\"\S+\"(?:@\w{2})*)(?:\s+;\s+(?:\S+:[^\s\}\]\)]+|a|\?\S+)\s+(?:\S+:[^\s\}\]\)\.]+|\?\S+|\"\S+\"(?:@\w{2})*))*'

        sparql_q_noComment= re.sub(r'#.*$', '', sparql_q, flags=re.MULTILINE) # Eliminazione commenti
        sparql_q_noVALUES = re.sub(VALUES_pattern, '', sparql_q_noComment) # Eliminazione clausole VALUES
        triples_matches = re.findall(triples_pattern, sparql_q_noVALUES, re.IGNORECASE)
        return triples_matches


    def get_subj_props_objs(self, triple:str) -> Tuple[str, Tuple[str], Tuple[str]]:
        '''
        Funzione che trova il soggetto, predicato/i e oggetto/i nella stringa della tripla
        passata in input.

        Segue descrizione regex usate:
        - subj_pattern individua il soggetto nella tripla, banalmente perch√© all'inizio,
        ma si specifica comunque che pu√≤ trattarsi di entit√† con prefisso o di variabile
        normalizzata e.g. ?var0
        - props_objs_pattern individua il/gli predicato/i e il/gli oggetto/i della tripla, catturando:
                - variabili normalizzate;
                - entit√† con prefissi;
                - literals con o senza lingua specificata.

        Input: match=stringa della tripla
        Output: Tupla con una stringa e due tuple in questo ordine: soggetto, tupla predicati e tupla oggetti
        '''

        subj_pattern = r'^(\?var\d+|\S+:\S+)'
        props_objs_pattern = r'(\S+:\S+|a|\?var\d+)\s+(\?var\d+|\S+:\S+|\"\S+\"(?:@\w{2})*)'
        subj = re.match(subj_pattern, triple, re.IGNORECASE)
        subj = subj.group()
        triple = triple[len(subj):] # Per evitare sovrapposizioni nel matching degli oggetti si elimina il soggetto dalla tripla
        props_objs_finds = re.findall(props_objs_pattern, triple, re.IGNORECASE)
        props, objs = zip(*props_objs_finds)
        return subj, props, objs


    def extract_triples(self, query: str) -> List[Triple]:
        """Extract triples from a SPARQL query"""
        triples = []
        triples_matches = self.retrieve_triples(query)
        if not triples_matches:
            return triples

        for match in triples_matches:
            subj, props, objs = self.get_subj_props_objs(match)
            triples.append(Triple(subj, props, objs))

        return triples


    def evaluate_dataset(
        self,
        input_queries: List[str],
        output_queries: List[str],
        predicted_queries: List[str]
    ) -> EvaluationMetrics:
        """Evaluate model performance in TSC"""
        assert len(input_queries) == len(output_queries) == len(predicted_queries)

        total_triples = 0
        correct_triples = 0
        flipped_triples_in_input = 0
        correctly_fixed = 0
        incorrectly_fixed = 0
        false_swaps = 0
        missed_flips = 0
        perfect_queries = 0

        for gold_q, input_q, pred_q in zip(output_queries, input_queries, predicted_queries):
            gold_triples = self.extract_triples(gold_q)
            input_triples = self.extract_triples(input_q)
            pred_triples = self.extract_triples(pred_q)

            if len(gold_triples) != len(pred_triples):
                continue

            total_triples += len(gold_triples)

            query_perfect = True
            input_swaps = set()
            pred_swaps = set()
            for i, (g_triple, i_triple, p_triple) in enumerate(zip(gold_triples, input_triples, pred_triples)):
                if not g_triple.subject == i_triple.subject:
                    if g_triple.subject in i_triple.objects and i_triple.subject in g_triple.objects:
                        input_swaps.add((i, i_triple.objects.index(g_triple.subject)))
                if not g_triple == p_triple:
                    if not g_triple.subject == p_triple.subject:
                        if g_triple.subject in p_triple.objects and p_triple.subject in g_triple.objects:
                            pred_swaps.add((i, p_triple.objects.index(g_triple.subject)))
                        else:
                          incorrectly_fixed += 1
                    query_perfect = False
                else:
                    correct_triples += 1

            flipped_triples_in_input += len(input_swaps)

            if query_perfect:
                perfect_queries += 1

            missed_flips += len(input_swaps & pred_swaps)
            set_false_swaps = pred_swaps - input_swaps
            false_swaps += len(set_false_swaps)
            correctly_fixed += len(input_swaps - (pred_swaps - set_false_swaps))

        triple_exact_match = correct_triples / total_triples if total_triples > 0 else 0
        correction_accuracy = (correctly_fixed / flipped_triples_in_input
                              if flipped_triples_in_input > 0 else 0)
        correct_triples_in_input = total_triples - flipped_triples_in_input
        preservation_accuracy = ((correct_triples - correctly_fixed) / correct_triples_in_input
                                if correct_triples_in_input > 0 else 0)

        true_positives = correctly_fixed
        false_positives = false_swaps
        precision = (true_positives / (true_positives + false_positives)
                    if (true_positives + false_positives) > 0 else 0)

        false_negatives = missed_flips
        recall = (true_positives / (true_positives + false_negatives)
                 if (true_positives + false_negatives) > 0 else 0)

        f1_score = (2 * precision * recall / (precision + recall)
                   if (precision + recall) > 0 else 0)

        query_exact_match = perfect_queries / len(output_queries)

        return EvaluationMetrics(
            triple_exact_match=triple_exact_match,
            correction_accuracy=correction_accuracy,
            preservation_accuracy=preservation_accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            query_exact_match=query_exact_match,
            total_triples=total_triples,
            correct_triples=correct_triples,
            flipped_triples_in_input=flipped_triples_in_input,
            correctly_fixed=correctly_fixed,
            incorrectly_fixed=incorrectly_fixed,
            false_swaps=false_swaps,
            missed_flips=missed_flips,
            perfect_queries=perfect_queries,
            total_queries=len(output_queries)
        )

    def print_metrics(self, metrics: EvaluationMetrics):
        """Pretty print evaluation metrics"""
        print("=" * 60)
        print("SPARQL TRIPLE CORRECTION EVALUATION RESULTS")
        print("=" * 60)

        print("\nüìä TRIPLE-LEVEL METRICS")
        print(f"  Triple Exact Match:      {metrics.triple_exact_match:.2%}")
        print(f"  Correction Accuracy:     {metrics.correction_accuracy:.2%}")
        print(f"  Preservation Accuracy:   {metrics.preservation_accuracy:.2%}")

        print("\nüéØ PRECISION, RECALL, F1")
        print(f"  Precision:               {metrics.precision:.2%}")
        print(f"  Recall:                  {metrics.recall:.2%}")
        print(f"  F1 Score:                {metrics.f1_score:.2%}")

        print("\nüìã QUERY-LEVEL METRICS")
        print(f"  Query Exact Match:       {metrics.query_exact_match:.2%}")
        print(f"  Perfect Queries:         {metrics.perfect_queries}/{metrics.total_queries}")

        print("\nüî¢ DETAILED COUNTS")
        print(f"  Total Triples:           {metrics.total_triples}")
        print(f"  Correct Triples:         {metrics.correct_triples}")
        print(f"  Flipped in Input:        {metrics.flipped_triples_in_input}")
        print(f"  Correctly Fixed:         {metrics.correctly_fixed}")
        print(f"  Missed Flips:            {metrics.missed_flips}")
        print(f"  False Swaps:             {metrics.false_swaps}")
        print(f"  Other Errors:            {metrics.incorrectly_fixed}")
        print("=" * 60)

'''Path al file di mapping wikidata id-label'''
IDS_LABELS_MAP_PATH = '/content/drive/MyDrive/Colab Notebooks/datasets/wikidata_ids_labels_map.json'

def apply_SB(name_dataset, serie:pd.Series) -> pd.Series:
    '''
    Funzione per applicare Semantic Backwarding per dataset Wikidata
    Per info vedi TSET (https://doi.org/10.3390/app14041521)

    La mia implementazione prevede l'uso di un json file dove reperire i labels,
    frutto di una precedente mappatura, per evitare di scaricare il dump di wikidata
    di oltre 130 GB.
    '''

    def _load_data():
      """Carica il file JSON se esiste, altrimenti restituisce un dict vuoto."""
      if os.path.exists(IDS_LABELS_MAP_PATH):
          with open(IDS_LABELS_MAP_PATH, "r", encoding="utf-8") as f:
              return json.load(f)
      return {}

    def _restore_wikidata_ids(text:str, dataset:Dict) -> str:
        '''
        Input: text=stringa di testo per domanda o sparql query
                dataset=dizionario del dataset ordinato secondo la lunghezza dei labels in ordine decresente
        Output stringa del testo modificata con id sostituiti dalle proprie labels
        '''

        new_text = text
        for id, label in dataset.items():
            new_text = new_text.replace(f':{label}', f':{id}')
        return new_text

    def _get_dataset(name_dataset):
        data = _load_data()
        return data.get(name_dataset, None)


    # Semantic Backwarding (SB)
    dataset_ids_labels_map = _get_dataset(name_dataset=name_dataset)

    if dataset_ids_labels_map is None:
        return print('Impossibile eseguire labeling degli id perch√© il dataset '\
                     'di riferimento non √® nel file json della mappatura.')

    ids_labels_map = dataset_ids_labels_map['ids_labels_map']
    # ID con labels nulli ignorati
    non_null_map = ((k,v) for k,v in ids_labels_map.items() if v is not None)
    # Ordine decrescente del dizionario secondo la lunghezza dei labels
    ids_labels_map = dict(sorted(non_null_map, key=lambda x: len(x[1]), reverse=True))

    print('\tApplicazione Semantic Backwarding...')
    print('\t\tApplicazione SB alla Series...')
    serie = serie.apply(lambda x: _restore_wikidata_ids(x, ids_labels_map)) # Applicazione SB all'oggetto Series
    print('\t\tProcesso concluso.')

    return serie

class CodeT5Evaluator:
    def __init__(self, model_path, base_model_path="Salesforce/codet5p-220m", tokenizer_path=None):
        """
        Inizializza l'evaluator con il modello fine-tuned con LoRA

        Args:
            model_path: path agli adapter LoRA salvati
            base_model_path: path o nome del modello base CodeT5+
            tokenizer_path: path al tokenizer (se None usa base_model_path)
        """

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # Carica tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path if tokenizer_path else base_model_path
        )

        # Carica modello base
        print(f"Loading base model from {base_model_path}")
        base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_path)

        # Carica adapter LoRA
        print(f"Loading LoRA adapters from {model_path}")
        self.model = PeftModel.from_pretrained(base_model, model_path)

        # Merge degli adapter per inference pi√π veloce (opzionale)
        self.model = self.model.merge_and_unload()

        self.model.to(self.device)
        self.model.eval()

    def generate_predictions(self, test_dataset, batch_size=8, max_length=256):
        """
        Genera predizioni per il dataset di test, che √® generato da
        LC-QuAD2.

        Args:
            test_dataset: dataset tokenizzato con colonne 'input_ids' e 'labels'
            batch_size: dimensione del batch
            max_length: lunghezza massima della generazione

        Returns:
            predictions: lista di stringhe predette
            references: lista di stringhe di riferimento
        """
        predictions = []
        references = []

        # Processa in batch
        for i in tqdm(range(0, len(test_dataset), batch_size), desc="Generating"):
            batch = test_dataset[i:i+batch_size]

            # Padding dinamico del batch
            max_input_len = max(len(ids) for ids in batch['input_ids'])
            padded_input_ids = []
            padded_attention_mask = []

            for ids, mask in zip(batch['input_ids'], batch['attention_mask']):
                padding_length = max_input_len - len(ids)
                padded_input_ids.append(ids + [self.tokenizer.pad_token_id] * padding_length)
                padded_attention_mask.append(mask + [0] * padding_length)

            # Prepara input
            input_ids = torch.tensor(padded_input_ids).to(self.device)
            attention_mask = torch.tensor(padded_attention_mask).to(self.device)

            # Genera predizioni
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=max_length,
                    num_beams=5,  # Beam search per migliori risultati
                    early_stopping=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            # Decodifica predizioni
            batch_preds = self.tokenizer.batch_decode(
                outputs, skip_special_tokens=True
            )
            predictions.extend(batch_preds)

            # Decodifica references con padding dinamico
            max_label_len = max(len(label) for label in batch['labels'])
            padded_labels = []

            for label in batch['labels']:
                # Sostituisci -100 con pad_token_id
                cleaned_label = [l if l != -100 else self.tokenizer.pad_token_id for l in label]
                padding_length = max_label_len - len(cleaned_label)
                padded_labels.append(cleaned_label + [self.tokenizer.pad_token_id] * padding_length)

            batch_refs = self.tokenizer.batch_decode(
                padded_labels, skip_special_tokens=True
            )
            references.extend(batch_refs)

        return predictions, references


    def evaluate(self, tokenized_test_dataset, test_dataset, batch_size=8, max_length=256, save_predictions=True):
        """
        Esegue valutazione completa

        Returns:
            dict con tutte le metriche
        """
        print("\n" + "="*50)
        print("Starting Evaluation")
        print("="*50)

        # Genera predizioni
        predictions, references = self.generate_predictions(
            tokenized_test_dataset, batch_size, max_length
        )

        # Salva predizioni (opzionale)
        if save_predictions:
            with open('TSC_predictions.txt', 'w', encoding='utf-8') as f:
                for pred, ref in zip(predictions, references):
                    f.write(f"REFERENCE: {ref}\n")
                    f.write(f"PREDICTION: {pred}\n")
                    f.write("-" * 80 + "\n")
            print("\nPredictions saved to 'TSC_predictions.txt'")

        # --- Create evaluator ---
        evaluator = SPARQLTripleEvaluator()

        # Calcola metriche
        input_dataset = apply_SB('LC-QuAD2', test_dataset['input']).tolist()
        output_dataset = apply_SB('LC-QuAD2', pd.Series(references)).tolist()
        predictions = apply_SB('LC-QuAD2', pd.Series(predictions)).tolist()

        metrics = evaluator.evaluate_dataset(input_dataset,
                                             output_dataset,
                                             predictions)

        # --- Print results ---
        evaluator.print_metrics(metrics)

        # --- Save results ---
        results_df = pd.DataFrame({
            'input': test_dataset['input'],
            'expected_output': test_dataset['output'],
            'predicted_output': predictions
        })
        results_df.to_csv('TSC_evaluation_results.csv', index=False)
        print("\n‚úÖ Results saved to 'TSC_evaluation_results.csv'")

        return metrics, predictions, references


def main():

    # 1. Caricamento modello fine-tuned con LoRA
    LORA_PATH = "/content/drive/MyDrive/Colab Notebooks/TSC/lora-codet5p"  # Path a adapter LoRA
    BASE_MODEL = "Salesforce/codet5p-220m"

    evaluator = CodeT5Evaluator(
        model_path=LORA_PATH,
        base_model_path=BASE_MODEL
    )

    # 2. Caricamento dataset di test
    # Opzione A: Dataset gi√† tokenizzato salvato localmente
    tokenized_test_dataset = load_from_disk("/content/drive/MyDrive/Colab Notebooks/datasets/TSC/LC-QuAD2/tokenized_test_dataset")

    # Opzione B: dataset da file JSON/CSV
    test_dataset = pd.read_json("/content/drive/MyDrive/Colab Notebooks/datasets/TSC/LC-QuAD2/test.json")

    # Opzione C: dataset HuggingFace
    # test_dataset = load_dataset("your_dataset", split="test")

    # 3. Esecuzione la valutazione
    results, predictions, references = evaluator.evaluate(
        tokenized_test_dataset=tokenized_test_dataset,
        test_dataset=test_dataset,
        batch_size=8,  # Da ridure se problemi di memoria
        max_length=256,
        save_predictions=True
    )

    # 4. Analisi esemlare di alcuni esempi
    print("\n" + "="*50)
    print("SAMPLE PREDICTIONS")
    print("="*50)
    for i in range(min(3, len(predictions))):
        print(f"\nExample {i+1}:")
        print(f"Reference:  {references[i][:100]}...")
        print(f"Prediction: {predictions[i][:100]}...")

    return results

# ============================================================================
# AVVIO TESTING
# ============================================================================

if __name__ == "__main__":
    # Assicurati di avere abbastanza memoria
    torch.cuda.empty_cache()

    # Esegui valutazione
    results = main()

"""# Inferenza manuale"""

inputs = tokenizer("sparql-to-sparql: PREFIX bd: <http:\/\/www.bigdata.com\/rdf#> PREFIX dct: <http:\/\/purl.org\/dc\/terms\/> PREFIX geo: <http:\/\/www.opengis.net\/ont\/geosparql#> PREFIX p: <http:\/\/www.wikidata.org\/prop\/> PREFIX pq: <http:\/\/www.wikidata.org\/prop\/qualifier\/> PREFIX ps: <http:\/\/www.wikidata.org\/prop\/statement\/> PREFIX psn: <http:\/\/www.wikidata.org\/prop\/statement\/value-normalized\/> PREFIX rdfs: <http:\/\/www.w3.org\/2000\/01\/rdf-schema#> PREFIX wd: <http:\/\/www.wikidata.org\/entity\/> PREFIX wds: <http:\/\/www.wikidata.org\/entity\/statement\/> PREFIX wdt: <http:\/\/www.wikidata.org\/prop\/direct\/> PREFIX wdv: <http:\/\/www.wikidata.org\/value\/> PREFIX wikibase: <http:\/\/wikiba.se\/ontology#> PREFIX xsd: <http:\/\/www.w3.org\/2001\/XMLSchema#> SELECT DISTINCT ?var0 {?var1 wdt:capital wd:Indonesia. ?var1 wdt:located in\/on physical feature ?var0. ?var0 wdt:instance of wd:island}", return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_length=128,
    num_beams=4,
    early_stopping=True
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))